# -*- coding: utf-8 -*-
# PIPELINE: get a list of data science institutions and their websites -> crawl through each of the listed organization's internal links and aggregate external links -> perform analysis
from urlparse import urlparse
from bs4 import BeautifulSoup
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy.http import Request
from scrapy.item import BaseItem
from scrapy.loader import ItemLoader
from scrapy.exceptions import CloseSpider
from labs.items import *
import scrapy
import csv
import json
import os
import datetime
import random

# DYNAMIC ITEM REFERNCE: (http://stackoverflow.com/questions/5069416/scraping-data-without-having-to-explicitly-define-each-field-to-be-scraped)
# Network Analysis Algorithms: https://networkx.github.io/documentation/latest/reference/algorithms.html

# Store different item into different collections in MongoDB
# processing different item in one pipeline: https://github.com/scrapy/scrapy/issues/102
class WebLabsSpider(scrapy.Spider):
    name = "weblabs"
    page_limit = 10000
    tier_limit = 5
    def __init__(self):
        self.filter_urls = list()
        self.requested_page_counter = dict()

    def start_requests(self):
        prefix = os.path.dirname(os.path.realpath(__file__))
        filename = "data-science-websites.csv"
        request_list = list()
        try:
            with open(os.path.join(prefix, filename), 'r') as csv_file:
                reader = csv.reader(csv_file)
                header = next(reader)
                for row in reader:
                    inst_name = row[0]
                    seed_url = row[1].strip()
                    base_url = urlparse(seed_url).netloc
                    self.filter_urls.append(base_url)
                    # Starts from 1 since the seed page will be always crawled
                    self.requested_page_counter[base_url] = 1
                    request = Request(seed_url, callback=self.parse_seed)
                    request.meta['inst_name'] = inst_name
                    request.meta['base_url'] = base_url
                    request_list.append(request)
                random.shuffle(request_list)
                return request_list
        except IOError:
            raise CloseSpider("A list of websites are needed")

    def parse_seed(self, response):
        inst_name = response.meta['inst_name']
        base_url = response.meta['base_url']

        # Meta value of depth is automatically generated by scrapy
        tier = response.meta['depth']
        priority = self.tier_limit - tier

        # handle external redirect while still allowing internal redirect
        if urlparse(response.url).netloc != base_url:
            return

        html_item = HTMLItem()
        html_item['inst_name'] = inst_name
        html_item['url'] = response.url
        html_item['base_url'] = base_url
        html_item['status'] = response.status
        html_item['headers'] = str(response.headers)
        html_item['body'] = response.body
        try:
            html_item['unicode_body'] = response.body_as_unicode()
        except AttributeError:
            pass
        html_item['request'] = str(response.request)
        html_item['timestamp'] = str(datetime.datetime.now())
        html_item['tier'] = tier
        yield html_item

        for internal_link in self.get_internal_links(base_url, response):
            if (tier >= self.tier_limit) or (self.requested_page_counter[base_url] >= self.page_limit):
                break
            self.requested_page_counter[base_url] += 1

            request = Request(internal_link.url, callback=self.parse_seed, priority=priority)

            request.meta['inst_name'] = inst_name
            request.meta['base_url'] = base_url
            request.meta['dont_redirect'] = True

            yield request

    def get_text(self, response):
        """ Extracts text portion from webpage"""
        text =  filter(None, [st.strip() for st in response.xpath("//*[not(self::script or self::style)]/text()[normalize-space()]").extract()])

        """
        soup = BeautifulSoup(response.body_as_unicode())
        # remove script (js) and style (css)
        for script in soup(["script", "style"]):
            script.extract()
        text = soup.get_text()
        ## break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        ## break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        ## drop blank lines
        text = ' '.join(chunk for chunk in chunks if chunk)
        """

        text = ' '.join(text)
        return text

    def get_external_links(self, base_url, response):
        return LinkExtractor(deny_domains=base_url).extract_links(response)

    def get_internal_links(self, base_url, response):
        return LinkExtractor(allow_domains=base_url, deny="/jenkins/").extract_links(response)

    def get_jenkins(self):
        return LinkExtractor(allow="/jenkins/").extract_links(response)
