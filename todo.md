
# Data Collection:

1. limit to 1000 webpages per website (**Don**)
2. run scrapy on the cloud to collect two kinds of data (**Alvin**)
    1. hyperlinks between websites (implemented)
    2. texts per websites (**Don**)

# Data Cleansing:

1. Remove irrelevant websites (Jenkins) -> Naive Bayes (**Don**)
OR
2. Find the source of institution descriptions (very difficult) 

# Data Analysis & Visualization:

- For hyperlinks
    1. visualization (D3.js) (**Vinitra**)
    2. network analysis

- For texts
    1. build LDA (**Louie** and **Don**)
    2. Word cloud 

