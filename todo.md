
Data Collection:
1. limit to 1000 webpages per website
2. run scrapy on the cloud to collect two kinds of data
    1. hyperlinks between websites (implemented)
    2. texts per websites

Data Cleansing:
1. Remove irrelevant websites (Jenkins) -> Naive Bayes
OR
2. Find the source of institution descriptions (very difficult) 

Data Analysis & Visualization :
- For hyperlinks
    1. visualization (D3.js)
    2. network analysis

- For texts
    1. build LDA
    2. Word cloud 

